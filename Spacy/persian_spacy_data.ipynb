{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c23f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 14:10:10.198406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 14:10:13.552854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.vocab import Vocab\n",
    "from tqdm import tqdm\n",
    "from hazm import PeykareReader\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789d9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "peykare = PeykareReader(root = '/repo/ebi/myPosTagger/resources/peykare/TextLabelData/',universal_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f69e312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344742"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(peykare.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f9dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 344736#len(list(peykare.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cefeae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('منبع', 'NOUN'),\n",
       " (':', 'PUNCT'),\n",
       " (')', 'PUNCT'),\n",
       " ('مجلة', 'NOUN,EZ'),\n",
       " ('سروش', 'NOUN,EZ'),\n",
       " ('هفتگی', 'ADJ'),\n",
       " ('،', 'PUNCT'),\n",
       " ('مصاحبه', 'NOUN'),\n",
       " ('با', 'ADP'),\n",
       " ('رئیس', 'NOUN,EZ'),\n",
       " ('دفتر', 'NOUN,EZ'),\n",
       " ('الجزیره', 'NOUN'),\n",
       " ('در', 'ADP'),\n",
       " ('تهران', 'NOUN'),\n",
       " ('،', 'PUNCT'),\n",
       " ('۱۳۸۰', 'NUM'),\n",
       " ('(', 'PUNCT')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(peykare.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee32ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_sents, test_sents = train_test_split(list(islice(peykare.sents(), corpus_size))\n",
    "                                           , test_size=0.1, random_state=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813a11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d265fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf5bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 310262/310262 [00:45<00:00, 6894.48it/s]\n",
      "tcmalloc: large alloc 1818607616 bytes == 0xbb8cc000 @ \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"fa\")\n",
    "\n",
    "# train_data = islice(peykare.sents(), int(corpus_size*0.95))\n",
    "\n",
    "\n",
    "db = DocBin()\n",
    "for sent in tqdm(train_sents):\n",
    "    words = [word[0] for word in sent]\n",
    "    tags = [word[1] for word in sent]\n",
    "    doc = Doc(Vocab(strings=words), words = words)\n",
    "    for d, tag in zip(doc, tags):\n",
    "        d.tag_ = tag\n",
    "    db.add(doc)\n",
    "    \n",
    "db.to_disk('../persian_spacy2/train_uni_randomState10.spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dca9894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 34474/34474 [00:04<00:00, 6898.06it/s]\n"
     ]
    }
   ],
   "source": [
    "db = DocBin()\n",
    "for sent in tqdm(test_sents):\n",
    "    words = [word[0] for word in sent]\n",
    "    tags = [word[1] for word in sent]\n",
    "    doc = Doc(Vocab(strings=words), words = words)\n",
    "    for d, tag in zip(doc, tags):\n",
    "        d.tag_ = tag\n",
    "    db.add(doc)\n",
    "db.to_disk('../persian_spacy2/test_uni_randomState10.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04bc7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_nlp = spacy.load('output2/model-last/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7511c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sent = 'در ارزش‌گذاری اسلامی و انسانی، ‌هیچ تفاوتی میان جنس زن و مرد نیست.'\n",
    "for sent in islice(test_data, 10):\n",
    "    doc = persian_nlp(' '.join([w for w, tag in sent]))\n",
    "    for y, x in zip(sent, doc):\n",
    "        if y[1] == x.tag_:\n",
    "            print(1)\n",
    "            print(y[0], y[1])\n",
    "        else:\n",
    "            print(y[0], f'[{y[1]}]', x.tag_) \n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b614c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34474it [32:48, 17.51it/s]                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADV       0.93      0.95      0.94     15492\n",
      "        ADVe       0.95      0.96      0.96      1114\n",
      "          AJ       0.93      0.95      0.94     57871\n",
      "         AJe       0.91      0.90      0.90     14645\n",
      "          CL       0.80      0.86      0.83      1349\n",
      "        CONJ       1.00      1.00      1.00     74057\n",
      "       CONJe       1.00      0.91      0.95        74\n",
      "         DET       0.98      0.97      0.98     18171\n",
      "        DETe       0.90      0.93      0.92      2165\n",
      "         INT       0.96      0.87      0.92        31\n",
      "           N       0.97      0.95      0.96    152057\n",
      "         NUM       0.98      0.99      0.99     17964\n",
      "        NUMe       0.89      0.93      0.91      1690\n",
      "          Ne       0.97      0.97      0.97    119251\n",
      "           P       1.00      1.00      1.00     70915\n",
      "       POSTP       1.00      1.00      1.00     12911\n",
      "         PRO       0.98      0.98      0.98     23117\n",
      "        PROe       0.87      0.83      0.85       326\n",
      "        PUNC       1.00      1.00      1.00     82510\n",
      "          Pe       0.97      0.98      0.98      8590\n",
      "         RES       0.91      0.94      0.92      6803\n",
      "        RESe       0.78      0.17      0.27       168\n",
      "           V       1.00      0.99      1.00     75711\n",
      "\n",
      "    accuracy                           0.97    756982\n",
      "   macro avg       0.94      0.91      0.92    756982\n",
      "weighted avg       0.97      0.97      0.97    756982\n",
      "\n",
      "Precision                                   : 0.975\n",
      "Recall                                      : 0.975\n",
      "F1-Score                                    : 0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "persian_nlp = spacy.load('output2/model-best/')\n",
    "test_data = islice(peykare.sents(), int(corpus_size*0.9) , corpus_size)\n",
    "preds = []\n",
    "ys = []\n",
    "\n",
    "for sent in tqdm(test_data, total=int(corpus_size*0.1)):\n",
    "    doc = persian_nlp(' '.join([w for w, tag in sent]))\n",
    "    y = [tag for w, tag in sent]\n",
    "    pred = [w.tag_ for w in doc]\n",
    "    if len(y) == len(pred):\n",
    "        ys += y\n",
    "        preds += pred\n",
    "\n",
    "\n",
    "print(classification_report(ys, preds))\n",
    "\n",
    "print('Precision                                   : %.3f'%precision_score(ys, preds, average='weighted'))\n",
    "print('Recall                                      : %.3f'%recall_score(ys, preds, average='weighted'))\n",
    "print('F1-Score                                    : %.3f'%f1_score(ys, preds, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f2044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy                                   : 0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy                                   : %.3f'%accuracy_score(ys, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e4aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████▊                          | 26281/34473 [30:45<10:34, 12.91it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "34474it [38:09, 15.06it/s]                                                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADV       0.93      0.95      0.94     15485\n",
      "        ADVe       0.95      0.96      0.96      1114\n",
      "          AJ       0.93      0.95      0.94     57871\n",
      "         AJe       0.91      0.90      0.90     14645\n",
      "          CL       0.80      0.86      0.83      1349\n",
      "        CONJ       1.00      1.00      1.00     74055\n",
      "       CONJe       1.00      0.91      0.95        74\n",
      "         DET       0.98      0.97      0.98     18177\n",
      "        DETe       0.90      0.93      0.92      2165\n",
      "         INT       0.96      0.87      0.92        31\n",
      "           N       0.97      0.95      0.96    152057\n",
      "         NUM       0.98      0.99      0.99     17963\n",
      "        NUMe       0.89      0.93      0.91      1690\n",
      "          Ne       0.97      0.97      0.97    119251\n",
      "           P       1.00      1.00      1.00     70919\n",
      "       POSTP       1.00      1.00      1.00     12911\n",
      "         PRO       0.98      0.98      0.98     23117\n",
      "        PROe       0.87      0.83      0.85       326\n",
      "        PUNC       1.00      1.00      1.00     82510\n",
      "          Pe       0.97      0.98      0.98      8590\n",
      "         RES       0.91      0.94      0.92      6803\n",
      "        RESe       0.78      0.17      0.27       168\n",
      "           V       1.00      0.99      1.00     75711\n",
      "\n",
      "    accuracy                           0.97    756982\n",
      "   macro avg       0.94      0.91      0.92    756982\n",
      "weighted avg       0.97      0.97      0.97    756982\n",
      "\n",
      "Precision                                   : 0.942\n",
      "Recall                                      : 0.914\n",
      "F1-Score                                    : 0.919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "persian_nlp = spacy.load('output2/model-best/')\n",
    "test_data = islice(peykare.sents(), int(corpus_size*0.9) , corpus_size)\n",
    "preds = []\n",
    "ys = []\n",
    "\n",
    "for sent in tqdm(test_data, total=int(corpus_size*0.1)):\n",
    "    doc = persian_nlp(' '.join([w for w, tag in sent]))\n",
    "    y = [tag for w, tag in sent]\n",
    "    pred = [w.tag_ for w in doc]\n",
    "    if len(y) == len(pred):\n",
    "        ys += y\n",
    "        preds += pred\n",
    "\n",
    "    \n",
    "print(classification_report(ys, preds))\n",
    "\n",
    "print('Precision                                   : %.3f'%precision_score(ys, preds, average='macro'))\n",
    "print('Recall                                      : %.3f'%recall_score(ys, preds, average='macro'))\n",
    "print('F1-Score                                    : %.3f'%f1_score(ys, preds, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6116664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34474it [01:12, 477.56it/s]                                                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADV       0.97      0.96      0.97     18901\n",
      "        ADVe       1.00      0.99      0.99      1376\n",
      "          AJ       0.97      0.96      0.97     68208\n",
      "         AJe       0.94      0.95      0.94     17071\n",
      "          CL       0.93      0.94      0.93      1578\n",
      "        CONJ       1.00      1.00      1.00     92009\n",
      "       CONJe       1.00      1.00      1.00        96\n",
      "         DET       0.98      0.97      0.98     22226\n",
      "        DETe       0.97      0.97      0.97      2661\n",
      "         INT       0.98      0.97      0.98        62\n",
      "           N       0.98      0.97      0.97    182593\n",
      "         NUM       0.99      0.99      0.99     20194\n",
      "        NUMe       0.95      0.93      0.94      1877\n",
      "          Ne       0.97      0.98      0.97    139753\n",
      "           P       1.00      1.00      1.00     84876\n",
      "       POSTP       1.00      1.00      1.00     16152\n",
      "         PRO       0.99      0.99      0.99     29719\n",
      "        PROe       0.96      0.92      0.94       391\n",
      "        PUNC       1.00      1.00      1.00     98913\n",
      "          Pe       0.98      0.99      0.99     10038\n",
      "         RES       0.99      0.99      0.99      8219\n",
      "        RESe       0.96      0.87      0.91       178\n",
      "           V       1.00      1.00      1.00     92975\n",
      "\n",
      "    accuracy                           0.98    910066\n",
      "   macro avg       0.98      0.97      0.97    910066\n",
      "weighted avg       0.98      0.98      0.98    910066\n",
      "\n",
      "Precision                                   : 0.978\n",
      "Recall                                      : 0.971\n",
      "F1-Score                                    : 0.974\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "test_data = islice(peykare.sents(), int(corpus_size*0.9) , corpus_size)\n",
    "preds = []\n",
    "ys = []\n",
    "tagger = POSTagger(model='/home/sobhe/baaz/resources/postagger.model')\n",
    "\n",
    "for sent in tqdm(test_data, total=int(corpus_size*0.1)):\n",
    "    tagged = tagger.tag([w for w, tag in sent])\n",
    "    y = [tag for w, tag in sent]\n",
    "    pred = [tag for w, tag in tagged]\n",
    "    if len(y) == len(pred):\n",
    "        ys += y\n",
    "        preds += pred\n",
    "        \n",
    "print(classification_report(ys, preds))\n",
    "\n",
    "print('Precision                                   : %.3f'%precision_score(ys, preds, average='macro'))\n",
    "print('Recall                                      : %.3f'%recall_score(ys, preds, average='macro'))\n",
    "print('F1-Score                                    : %.3f'%f1_score(ys, preds, average='macro'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "525cd03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فکر \n",
      "می‌کنم \n",
      "بهترین \n",
      "مسئله \n",
      "درباره‌ی \n",
      "تنها \n",
      "کار \n",
      "کردن \n",
      "اینست \n",
      "که \n",
      "تصور \n",
      "شخصیت \n",
      "را \n",
      "دنبال \n",
      "می‌کنی \n",
      "، \n",
      "احساس \n",
      "خودت \n",
      "را \n",
      "دنبال \n",
      "می‌کنی \n",
      "و \n",
      "شخصیتها \n",
      "درست \n",
      "از \n",
      "تو \n",
      "هستند \n",
      ". \n",
      "به‌طورکلی \n",
      "روابط \n",
      "خوبی \n",
      "با \n",
      "همکاران \n",
      "نویسنده‌ام \n",
      "داشته‌ام \n",
      ". \n",
      "دوست \n",
      "دارم \n",
      "تنها \n",
      "بنویسم \n",
      "و \n",
      "اگر \n",
      "ایده‌ام \n",
      "با \n",
      "یک \n",
      "همکار \n",
      "جور \n",
      "باشد \n",
      "، \n",
      "با \n",
      "او \n",
      "به \n",
      "صورت \n",
      "مشترک \n",
      "بنویسم \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    items = next(y)\n",
    "    words = [word[0] for word in items]\n",
    "    tags = [word[1] for word in items]\n",
    "    doc = Doc(Vocab(strings=words), words = words)\n",
    "    for d, tag in zip(doc, tags):\n",
    "        print(d.text, d.pos_)\n",
    "        db.add(doc)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f613f0d8",
   "metadata": {},
   "source": [
    "## output2/model-last\n",
    "\n",
    "Precision                                   : 0.975\n",
    "Recall                                      : 0.975\n",
    "F1-Score                                    : 0.975\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "         ADV       0.93      0.95      0.94     15492\n",
    "        ADVe       0.95      0.96      0.96      1114\n",
    "          AJ       0.93      0.95      0.94     57871\n",
    "         AJe       0.91      0.90      0.90     14645\n",
    "          CL       0.80      0.86      0.83      1349\n",
    "        CONJ       1.00      1.00      1.00     74057\n",
    "       CONJe       1.00      0.91      0.95        74\n",
    "         DET       0.98      0.97      0.98     18170\n",
    "        DETe       0.90      0.93      0.92      2165\n",
    "         INT       0.96      0.87      0.92        31\n",
    "           N       0.97      0.95      0.96    152083\n",
    "         NUM       0.98      0.99      0.99     17964\n",
    "        NUMe       0.89      0.93      0.91      1690\n",
    "          Ne       0.97      0.97      0.97    119267\n",
    "           P       1.00      1.00      1.00     70890\n",
    "       POSTP       1.00      1.00      1.00     12911\n",
    "         PRO       0.98      0.98      0.98     23117\n",
    "        PROe       0.87      0.83      0.85       326\n",
    "        PUNC       1.00      1.00      1.00     82510\n",
    "          Pe       0.97      0.98      0.98      8574\n",
    "         RES       0.91      0.94      0.92      6803\n",
    "        RESe       0.78      0.17      0.27       168\n",
    "           V       1.00      0.99      1.00     75711\n",
    "\n",
    "    accuracy                           0.97    756982\n",
    "   macro avg       0.94      0.91      0.92    756982\n",
    "weighted avg       0.97      0.97      0.97    756982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56950424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc903dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae00a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c521c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e0213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5a31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779872e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"این ماژول شامل کلاس‌ها و توابعی برای خواندن پیکرهٔ Peykare است. \n",
    "[peykare پیکرهٔ](https://www.peykaregan.ir/dataset/%D9%BE%DB%8C%DA%A9%D8%B1%D9%87-%D9%85%D8%AA%D9%86%DB%8C-%D8%B2%D8%A8%D8%A7%D9%86-%D9%81%D8%A7%D8%B1%D8%B3%DB%8C)\n",
    "جموعه‌ای از متون نوشتاری و گفتاری رسمی زبان فارسی است که از منابع واقعی همچون\n",
    "روزنامه‌ها، سایت‌ها و مستنداتِ از قبل تایپ‌شده، جمع‌آوری شده، تصحیح گردیده و\n",
    "برچسب خورده است. حجم این دادگان حدوداً ۱۰۰ میلیون کلمه است و از منابع مختلف تهیه\n",
    "گردیده و دارای تنوع بسیار زیادی است. ۱۰ میلیون کلمه از این پیکره با استفاده از\n",
    "۸۸۲ برچسب نحوی-معنایی به صورت دستی توسط دانشجویان رشتهٔ زبان‌شناسی برچسب‌دهی\n",
    "شده‌اند و هر پرونده بر حسب موضوع و منبع آن طبقه‌بندی شده است. این پیکره که توسط\n",
    "پژوهشکده پردازش هوشمند علائم تهیه شده است، برای استفاده در آموزش مدل زبانی و\n",
    "سایر پروژه‌های مربوط به پردازش زبان طبیعی مناسب است.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import os, codecs\n",
    "from hazm import Normalizer\n",
    "from hazm import WordTokenizer\n",
    "\n",
    "\n",
    "def coarse_pos_u(tags, word):\n",
    "    \"\"\"برچسب‌های ریز را به برچسب‌های درشت منطبق با استاندارد جهانی (coarse-grained universal pos tags) تبدیل می‌کند.\n",
    "    Examples:\n",
    "            >>> coarse_pos_u(['N','COM','SING'], 'الجزیره')\n",
    "            'NOUN'\n",
    "    Args:\n",
    "            tags (List[str]): لیست برچسب‌های ریز.\n",
    "    Returns:\n",
    "            (List[str]): لیست برچسب‌های درشت جهانی.\n",
    "    \"\"\"\n",
    "\n",
    "    map_pos_to_upos = {\n",
    "        \"N\": \"NOUN\",\n",
    "        \"V\": \"VERB\",\n",
    "        \"AJ\": \"ADJ\",\n",
    "        \"ADV\": \"ADV\",\n",
    "        \"PRO\": \"PRON\",\n",
    "        \"DET\": \"DET\",\n",
    "        \"P\": \"ADP\",\n",
    "        \"POSTP\": \"ADP\",\n",
    "        \"NUM\": \"NUM\",\n",
    "        \"CONJ\": \"CCONJ\",\n",
    "        \"PUNC\": \"PUNCT\",\n",
    "        \"CL\": \"NOUN\",\n",
    "        \"INT\": \"INTJ\",\n",
    "        \"RES\": \"NOUN\",\n",
    "    }\n",
    "    sconj_list = {\n",
    "        \"که\",\n",
    "        \"تا\",\n",
    "        \"گرچه\",\n",
    "        \"اگرچه\",\n",
    "        \"چرا\",\n",
    "        \"زیرا\",\n",
    "        \"اگر\",\n",
    "        \"چون\",\n",
    "        \"چراکه\",\n",
    "        \"هرچند\",\n",
    "        \"وگرنه\",\n",
    "        \"چنانچه\",\n",
    "        \"والا\",\n",
    "        \"هرچه\",\n",
    "        \"ولو\",\n",
    "        \"مگر\",\n",
    "        \"پس\",\n",
    "        \"چو\",\n",
    "        \"چه\",\n",
    "        \"بنابراین\",\n",
    "        \"وقتی\",\n",
    "        \"والّا\",\n",
    "        \"انگاری\",\n",
    "        \"هرچندكه\",\n",
    "        \"درنتيجه\",\n",
    "        \"اگه\",\n",
    "        \"ازآنجاكه\",\n",
    "        \"گر\",\n",
    "        \"وگر\",\n",
    "        \"وقتيكه\",\n",
    "        \"تااينكه\",\n",
    "        \"زمانيكه\",\n",
    "    }\n",
    "    num_adj_list = {\n",
    "        \"نخست\",\n",
    "        \"دوم\",\n",
    "        \"اول\",\n",
    "        \"پنجم\",\n",
    "        \"آخر\",\n",
    "        \"يازدهم\",\n",
    "        \"نهم\",\n",
    "        \"چهارم\",\n",
    "        \"ششم\",\n",
    "        \"پانزدهم\",\n",
    "        \"دوازدهم\",\n",
    "        \"هشتم\",\n",
    "        \"صدم\",\n",
    "        \"هفتم\",\n",
    "        \"هفدهم\",\n",
    "        \"آخرين\",\n",
    "        \"سيزدهم\",\n",
    "        \"يكم\",\n",
    "        \"بيستم\",\n",
    "        \"ويكم\",\n",
    "        \"دوسوم\",\n",
    "        \"شانزدهم\",\n",
    "        \"هجدهم\",\n",
    "        \"چهاردهم\",\n",
    "        \"ششصدم\",\n",
    "        \"ميليونيم\",\n",
    "        \"وهفتم\",\n",
    "        \"يازدهمين\",\n",
    "        \"هيجدهمين\",\n",
    "        \"واپسين\",\n",
    "        \"چهلم\",\n",
    "        \"هزارم\",\n",
    "        \"وپنجم\",\n",
    "        \"هيجدهم\",\n",
    "        \"ميلياردم\",\n",
    "        \"ميليونيوم\",\n",
    "        \"تريليونيوم\",\n",
    "        \"چهارپنجم\",\n",
    "        \"دهگانه\",\n",
    "        \"ميليونم\",\n",
    "        \"اوّل\",\n",
    "        \"سوّم\",\n",
    "    }\n",
    "    try:\n",
    "        old_pos = list(\n",
    "            set(tags)\n",
    "            & {\n",
    "                \"N\",\n",
    "                \"V\",\n",
    "                \"AJ\",\n",
    "                \"ADV\",\n",
    "                \"PRO\",\n",
    "                \"DET\",\n",
    "                \"P\",\n",
    "                \"POSTP\",\n",
    "                \"NUM\",\n",
    "                \"CONJ\",\n",
    "                \"PUNC\",\n",
    "                \"CL\",\n",
    "                \"INT\",\n",
    "                \"RES\",\n",
    "            }\n",
    "        )[0] \n",
    "        if old_pos == \"CONJ\" and word in sconj_list:\n",
    "            return \"SCONJ\"\n",
    "        if old_pos == \"NUM\" and word in num_adj_list:\n",
    "            return \"ADJ\" + (\",EZ\" if \"EZ\" in tags else \"\")\n",
    "        return map_pos_to_upos[old_pos] + (\",EZ\" if \"EZ\" in tags else \"\")\n",
    "    except:\n",
    "        return \"NOUN\"\n",
    "\n",
    "\n",
    "def coarse_pos_e(tags, word):\n",
    "    \"\"\"برچسب‌های ریز را به برچسب‌های درشت (coarse-grained pos tags) تبدیل می‌کند.\n",
    "    Examples:\n",
    "            >>> coarse_pos_e(['N','COM','SING'], 'الجزیره')\n",
    "            'N'\n",
    "    Args:\n",
    "            tags (List[str]): لیست برچسب‌های ریز.\n",
    "    Returns:\n",
    "            (List[str]): لیست برچسب‌های درشت.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return list(\n",
    "            set(tags)\n",
    "            & {\n",
    "                \"N\",\n",
    "                \"V\",\n",
    "                \"AJ\",\n",
    "                \"ADV\",\n",
    "                \"PRO\",\n",
    "                \"DET\",\n",
    "                \"P\",\n",
    "                \"POSTP\",\n",
    "                \"NUM\",\n",
    "                \"CONJ\",\n",
    "                \"PUNC\",\n",
    "                \"CL\",\n",
    "                \"INT\",\n",
    "                \"RES\",\n",
    "            }\n",
    "        )[0] + (\",EZ\" if \"EZ\" in tags else \"\")\n",
    "    except:\n",
    "        return \"N\"\n",
    "\n",
    "\n",
    "def join_verb_parts(sentence):\n",
    "    \"\"\"جمله را در قالب لیستی از `(توکن، برچسب)‌`ها می‌گیرد و توکن‌های مربوط به افعال چندبخشی را با کاراکتر زیرخط (_) به هم می‌چسباند.\n",
    "    Examples:\n",
    "            >>> join_verb_parts([('اولین', 'AJ'), ('سیاره', 'Ne'), ('خارج', 'AJ'), ('از', 'P'), ('منظومه', 'Ne'), ('شمسی', 'AJ'), ('دیده', 'AJ'), ('شد', 'V'), ('.', 'PUNC')])\n",
    "            [('اولین', 'AJ'), ('سیاره', 'Ne'), ('خارج', 'AJ'), ('از', 'P'), ('منظومه', 'Ne'), ('شمسی', 'AJ'), ('دیده_شد', 'V'), ('.', 'PUNC')]\n",
    "    Args:\n",
    "            sentence(List[Tuple[str,str]]): جمله در قالب لیستی از `(توکن، برچسب)`ها.\n",
    "    Returns:\n",
    "            (List[Tuple[str, str]): لیستی از `(توکن، برچسب)`ها که در آن افعال چندبخشی در قالب یک توکن با کاراکتر زیرخط به هم چسبانده شده‌اند.\n",
    "    \"\"\"\n",
    "\n",
    "    if not hasattr(join_verb_parts, \"tokenizer\"):\n",
    "        join_verb_parts.tokenizer = WordTokenizer()\n",
    "    before_verbs, after_verbs, verbe = (\n",
    "        join_verb_parts.tokenizer.before_verbs,\n",
    "        join_verb_parts.tokenizer.after_verbs,\n",
    "        join_verb_parts.tokenizer.verbe,\n",
    "    )\n",
    "\n",
    "    result = [(\"\", \"\")]\n",
    "    for word in reversed(sentence):\n",
    "        if word[0] in before_verbs or (\n",
    "            result[-1][0] in after_verbs and word[0] in verbe\n",
    "        ):\n",
    "            result[-1] = (word[0] + \"_\" + result[-1][0], result[-1][1])\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return list(reversed(result[1:]))\n",
    "\n",
    "\n",
    "class PeykareReader:\n",
    "    \"\"\"این کلاس شامل توابعی برای خواندن پیکرهٔ Peykare است.\n",
    "    Args:\n",
    "            root (str): آدرس فولدر حاوی فایل‌های پیکره.\n",
    "            join_verb_parts (bool, optional): اگر `True‍` باشد افعال چندقسمتی به‌شکل چسبیده‌به‌هم برگردانده_می‌شود.\n",
    "            pos_map (str): دیکشنری مبدل برچسب‌های ریز به درشت.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, joined_verb_parts=True, pos_map=coarse_pos_e, universal_pos=False\n",
    "    ):\n",
    "        self._root = root\n",
    "        if pos_map is None:\n",
    "            self._pos_map = lambda tags: \",\".join(tags)\n",
    "        elif universal_pos:\n",
    "            self._pos_map = coarse_pos_u\n",
    "        else:\n",
    "            self._pos_map = coarse_pos_e\n",
    "        self._joined_verb_parts = joined_verb_parts\n",
    "        self._normalizer = Normalizer(punctuation_spacing=False, affix_spacing=False)\n",
    "\n",
    "    def docs(self):\n",
    "        \"\"\"اسناد را به شکل متن خام برمی‌گرداند.\n",
    "        Yields:\n",
    "                (str): متن خام سند بعدی.\n",
    "        \"\"\"\n",
    "\n",
    "        for root, dirs, files in os.walk(self._root):\n",
    "            for name in sorted(files):\n",
    "                with codecs.open(\n",
    "                    os.path.join(root, name), encoding=\"windows-1256\"\n",
    "                ) as peykare_file:\n",
    "                    text = peykare_file.read()\n",
    "                    if text:\n",
    "                        yield text\n",
    "\n",
    "    def doc_to_sents(self, document):\n",
    "        \"\"\"سند ورودی را به لیستی از جملات تبدیل می‌کند.\n",
    "        هر جمله لیستی از `(کلمه, برچسب)`ها است.\n",
    "        Args:\n",
    "                document (str): سندی که باید تبدیل شود.\n",
    "        Yields:\n",
    "                (List[(str,str)]): `ها جملهٔ بعدی در قالب لیستی از `(کلمه، برچسب).\n",
    "        \"\"\"\n",
    "\n",
    "        sentence = []\n",
    "        for line in document.split(\"\\r\\n\"):\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(\" \")\n",
    "            tags, word = parts[3], self._normalizer.normalize(\"‌\".join(parts[4:]))\n",
    "\n",
    "            if word and word != \"#\":\n",
    "                sentence.append((word, tags))\n",
    "\n",
    "            if parts[2] == \"PUNC\" and word in {\"#\", \".\", \"؟\", \"!\"}:\n",
    "                if len(sentence) > 1:\n",
    "                    yield sentence\n",
    "                sentence = []\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"جملات پیکره را در قالب لیستی از `(توکن، برچسب)`ها برمی‌گرداند.\n",
    "        Examples:\n",
    "                >>> peykare = PeykareReader(root='corpora/peykare')\n",
    "                >>> next(peykare.sents())\n",
    "                [('دیرزمانی', 'N'), ('از', 'P'), ('راه‌اندازی', 'Ne'), ('شبکه‌ی', 'Ne'), ('خبر', 'Ne'), ('الجزیره', 'N'), ('نمی‌گذرد', 'V'), ('،', 'PUNC'), ('اما', 'CONJ'), ('این', 'DET'), ('شبکه‌ی', 'Ne'), ('خبری', 'AJe'), ('عربی', 'N'), ('بسیار', 'ADV'), ('سریع', 'ADV'), ('توانسته', 'V'), ('در', 'P'), ('میان', 'Ne'), ('شبکه‌های', 'Ne'), ('عظیم', 'AJe'), ('خبری', 'AJ'), ('و', 'CONJ'), ('بنگاه‌های', 'Ne'), ('چندرسانه‌ای', 'AJe'), ('دنیا', 'N'), ('خودی', 'N'), ('نشان', 'N'), ('دهد', 'V'), ('.', 'PUNC')]\n",
    "                >>> peykare = PeykareReader(root='corpora/peykare', joined_verb_parts=False, pos_map=None)\n",
    "                >>> next(peykare.sents())\n",
    "                [('دیرزمانی', 'N,COM,SING,TIME,YA'), ('از', 'P'), ('راه‌اندازی', 'N,COM,SING,EZ'), ('شبکه‌ی', 'N,COM,SING,EZ'), ('خبر', 'N,COM,SING,EZ'), ('الجزیره', 'N,PR,SING'), ('نمی‌گذرد', 'V,PRES,NEG,3'), ('،', 'PUNC'), ('اما', 'CONJ'), ('این', 'DET,DEMO'), ('شبکه‌ی', 'N,COM,SING,EZ'), ('خبری', 'AJ,SIM,EZ'), ('عربی', 'N,PR,SING'), ('بسیار', 'ADV,INTSF,SIM'), ('سریع', 'ADV,GENR,SIM'), ('توانسته', 'V,PASTP'), ('در', 'P'), ('میان', 'N,COM,SING,EZ'), ('شبکه‌های', 'N,COM,PL,EZ'), ('عظیم', 'AJ,SIM,EZ'), ('خبری', 'AJ,SIM'), ('و', 'CONJ'), ('بنگاه‌های', 'N,COM,PL,EZ'), ('چندرسانه‌ای', 'AJ,SIM,EZ'), ('دنیا', 'N,COM,SING'), ('خودی', 'N,COM,SING,YA'), ('نشان', 'N,COM,SING'), ('دهد', 'V,SUB,POS,3'), ('.', 'PUNC')]\n",
    "        Yields:\n",
    "                (List[Tuple[str,str]]): جملهٔ بعدی در قالب لیستی از `(توکن، برچسب)`ها.\n",
    "        \"\"\"\n",
    "        map_pos = lambda item: (item[0], self._pos_map(item[1].split(\",\"), item[0]))\n",
    "\n",
    "        for document in self.docs():\n",
    "            for sentence in self.doc_to_sents(document):\n",
    "                if self._joined_verb_parts:\n",
    "                    sentence = join_verb_parts(sentence)\n",
    "                yield list(map(map_pos, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "981595e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peykare = PeykareReader(root = '/repo/ebi/myPosTagger/resources/peykare/TextLabelData/', universal_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26579e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('منبع', 'N,COM,SING'), (':', 'PUNC'), (')', 'PUNC'), ('مجلة', 'N,COM,SING,EZ'), ('سروش', 'N,PR,SING,EZ'), ('هفتگی', 'AJ,SIM'), ('،', 'PUNC'), ('مصاحبه', 'N,COM,SING'), ('با', 'P'), ('رئیس', 'N,COM,SING,EZ'), ('دفتر', 'N,COM,SING,LOC,EZ'), ('الجزیره', 'N,PR,SING'), ('در', 'P'), ('تهران', 'N,PR,SING,LOC'), ('،', 'PUNC'), ('۱۳۸۰', 'NUM,CAR,NOMI,SING'), ('(', 'PUNC')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('منبع', 'NOUN'),\n",
       " (':', 'PUNCT'),\n",
       " (')', 'PUNCT'),\n",
       " ('مجلة', 'NOUN,EZ'),\n",
       " ('سروش', 'NOUN,EZ'),\n",
       " ('هفتگی', 'ADJ'),\n",
       " ('،', 'PUNCT'),\n",
       " ('مصاحبه', 'NOUN'),\n",
       " ('با', 'ADP'),\n",
       " ('رئیس', 'NOUN,EZ'),\n",
       " ('دفتر', 'NOUN,EZ'),\n",
       " ('الجزیره', 'NOUN'),\n",
       " ('در', 'ADP'),\n",
       " ('تهران', 'NOUN'),\n",
       " ('،', 'PUNCT'),\n",
       " ('۱۳۸۰', 'NUM'),\n",
       " ('(', 'PUNCT')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(peykare.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec80aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
